{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: PEFT stands for Parameter-Efficient Fine-Tuning, is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. The technique implemented is called LoRA, (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained.\n",
    "\n",
    "* Model: The choosen model is distilbert, a successful language model that makes use of attention mechanism to improve it's performance.\n",
    "\n",
    "* Evaluation approach: Since this is a classification challenge I decided to monitor accuracy score, the total amount of correctly classified samples divided by the total number of samples. The Categorcal Cross Entropy Loss is also monitored, a function that captures discrepancy among real values and predictions.\n",
    "\n",
    "* Fine-tuning dataset: I choose a sentiment analysis dataset in the financial industry called Auditor Sentiment. Data can be found [here](https://huggingface.co/datasets/FinanceInc/auditor_sentiment) has the following description: ***Auditor sentiment dataset of sentences from financial news. The dataset consists of several thousand sentences from English language financial news categorized by sentiment.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 3.71k/3.71k [00:00<00:00, 3.17MB/s]\n",
      "Downloading metadata: 100%|██████████| 800/800 [00:00<00:00, 697kB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/327k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 327k/327k [00:00<00:00, 561kB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:00<00:00,  1.68it/s]\n",
      "Downloading data:   0%|          | 0.00/80.9k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 80.9k/80.9k [00:00<00:00, 216kB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1289.36it/s]\n",
      "Generating train split: 100%|██████████| 3877/3877 [00:00<00:00, 518388.11 examples/s]\n",
      "Generating test split: 100%|██████████| 969/969 [00:00<00:00, 386191.62 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label'],\n",
       "    num_rows: 3101\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load sentiment analysis financial data\n",
    "dataset = load_dataset(\n",
    "    \"FinanceInc/auditor_sentiment\", \n",
    "    split=\"train\").train_test_split(\n",
    "        test_size=0.2, \n",
    "        shuffle=True, \n",
    "        seed=23\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9fbb4",
   "metadata": {},
   "source": [
    "There are two different sets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49dd385",
   "metadata": {},
   "source": [
    "Data sizes for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions: 3101 rows and 2 columns.\n",
      "Testing set dimensions: 776 rows and 2 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set dimensions: {dataset['train'].shape[0]} rows and {dataset['train'].shape[1]} columns.\")\n",
    "print(f\"Testing set dimensions: {dataset['test'].shape[0]} rows and {dataset['test'].shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7fd55",
   "metadata": {},
   "source": [
    "Data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6728e1b",
   "metadata": {},
   "source": [
    "Checking some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cf4501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ---------------------------------------------------------------------- -------------- Munich , 14 January 2008 : BAVARIA Industriekapital AG closed the acquisition of Elcoteq Communications Technology GmbH in Offenburg , Germany , with the approval of the\n",
      "Label: 1\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: However , sales volumes in the food industry are expected to remain at relatively good levels in Finland and in Scandinavia , Atria said .\n",
      "Label: 2\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: The optimization of the steel components heating process will reduce the energy consumption .\n",
      "Label: 2\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: Each share is entitled to one vote .\n",
      "Label: 1\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: - Net sales for the period are expected to fall well below that of last year and the result after non-recurring items is expected to be in the red .\n",
      "Label: 0\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: The plant is scheduled for completion in late February 2007 with hand over of some areas in January Two other suppliers of Nokia - Aspocomp Group Oyj and Perlos - have announced their plans to establish plants within the Nokia complex Together , they will invest Rs 365 crore .\n",
      "Label: 1\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: Residents access to the block is planned to be from Aleksandri Street .\n",
      "Label: 1\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: A meeting for the media and analysts will be held on the same day at 10:30 a.m. at Stonesoft Headquarters in Helsinki , Italahdenkatu 22 A. The Interim report will be presented by Stonesoft 's CEO Ilkka Hiidenheimo .\n",
      "Label: 1\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: At present , the trade mark Pit-Produkt is little-known outside the North-West of Russia .\n",
      "Label: 0\n",
      "____________________________________________________________________________________________________\n",
      "Sentence: The report examines the medical equipment business structure and operations , history and products , and provides an analysis of its key medical equipment revenue lines .\n",
      "Label: 1\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Sentence: {dataset['train'][i]['sentence']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")\n",
    "    print(\"_\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e79bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 108kB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 2.21MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 6.27MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 24.5MB/s]\n",
      "Map: 100%|██████████| 3101/3101 [00:00<00:00, 6785.59 examples/s]\n",
      "Map: 100%|██████████| 776/776 [00:00<00:00, 6914.63 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3101\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Let's use a lambda function to tokenize all the examples\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sentence\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a5351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 268M/268M [00:01<00:00, 183MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# label: a label corresponding to the class as a string: 'positive' - (2), 'neutral' - (1), or 'negative' - (0)\n",
    "\n",
    "id2label_dict = {\n",
    "    2:'positive', \n",
    "    1: 'neutral', \n",
    "    0: 'negative'\n",
    "} \n",
    "\n",
    "label2id_dict = {\n",
    "    'positive':2, \n",
    "    'neutral':1, \n",
    "    'negative':0\n",
    "} \n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    id2label = id2label_dict,\n",
    "    label2id = label2id_dict,\n",
    ")\n",
    "\n",
    "    \n",
    "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a2c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3880' max='3880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3880/3880 02:51, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.867723</td>\n",
       "      <td>0.592784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.821184</td>\n",
       "      <td>0.597938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>0.776846</td>\n",
       "      <td>0.634021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>0.748419</td>\n",
       "      <td>0.641753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>0.720939</td>\n",
       "      <td>0.658505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.700930</td>\n",
       "      <td>0.672680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.688914</td>\n",
       "      <td>0.679124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.675683</td>\n",
       "      <td>0.694588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.669286</td>\n",
       "      <td>0.695876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.656425</td>\n",
       "      <td>0.701031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.645135</td>\n",
       "      <td>0.710052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.638687</td>\n",
       "      <td>0.715206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.635093</td>\n",
       "      <td>0.713918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.631086</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.628178</td>\n",
       "      <td>0.717784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.625673</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.623208</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.621751</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.621094</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.620438</td>\n",
       "      <td>0.716495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3880, training_loss=0.7070649294509102, metrics={'train_runtime': 172.6696, 'train_samples_per_second': 359.183, 'train_steps_per_second': 22.471, 'total_flos': 952228605710790.0, 'train_loss': 0.7070649294509102, 'epoch': 20.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# set the number of epochs in the experiment\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=16, \n",
    "        per_device_eval_batch_size=16, \n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d7230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6204381585121155,\n",
       " 'eval_accuracy': 0.7164948453608248,\n",
       " 'eval_runtime': 1.3257,\n",
       " 'eval_samples_per_second': 585.355,\n",
       " 'eval_steps_per_second': 36.962,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "            \n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3592898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 67,398,147 || trainable%: 0.6563503889802786\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3880' max='3880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3880/3880 04:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.558130</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.498131</td>\n",
       "      <td>0.792526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.463955</td>\n",
       "      <td>0.819588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.436276</td>\n",
       "      <td>0.829897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.421365</td>\n",
       "      <td>0.829897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.406964</td>\n",
       "      <td>0.835052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.403874</td>\n",
       "      <td>0.836340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.397858</td>\n",
       "      <td>0.836340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.388594</td>\n",
       "      <td>0.844072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.379351</td>\n",
       "      <td>0.840206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.375610</td>\n",
       "      <td>0.845361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.371992</td>\n",
       "      <td>0.851804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.370973</td>\n",
       "      <td>0.849227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.368008</td>\n",
       "      <td>0.850515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.368019</td>\n",
       "      <td>0.850515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.366297</td>\n",
       "      <td>0.851804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.849227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.367500</td>\n",
       "      <td>0.361648</td>\n",
       "      <td>0.850515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>0.850515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.360748</td>\n",
       "      <td>0.851804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3880, training_loss=0.4185263800866825, metrics={'train_runtime': 270.882, 'train_samples_per_second': 228.956, 'train_steps_per_second': 14.324, 'total_flos': 961997139562950.0, 'train_loss': 0.4185263800866825, 'epoch': 20.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=16, \n",
    "        per_device_eval_batch_size=16, \n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3607477843761444,\n",
       " 'eval_accuracy': 0.8518041237113402,\n",
       " 'eval_runtime': 1.4421,\n",
       " 'eval_samples_per_second': 538.11,\n",
       " 'eval_steps_per_second': 33.979,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0ba45",
   "metadata": {},
   "source": [
    "Accuracy with PEFT in the test set improves from 71.64% to 85.05%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67557d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"lora_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "270a9f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "LightweightFineTuning.ipynb  data  logs  lora_model  train_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01fb5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig, AutoPeftModelForSequenceClassification, AutoPeftModelForCausalLM\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "peft_model_id = \"lora_model\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da397097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1aee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not execute! thorows an error D:\n",
    "#model_from_disk = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "#    config.base_model_name_or_path,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7912cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading model with AutoModelForSequenceClassification since peft doesn't work\n",
    "model_from_disk = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98489305",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataCollatorForMultipleChoice' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForMultipleChoice\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataCollatorForMultipleChoice' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecafbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_loaded_model = Trainer(\n",
    "    model=model,\n",
    "    #data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer), \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c3a72a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Efore plant at Saarijarvi in central Finla...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1996, 1041, 29278, 2063, 3269, 2012, 784...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The companies will divest to UPM Fray Bentos p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1996, 3316, 2097, 11529, 3367, 2000, 203...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The tower 's engineers have created an 18 degr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1996, 3578, 1005, 1055, 6145, 2031, 2580...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finnish silicon wafer technology company Okmet...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 6983, 13773, 11333, 7512, 2974, 2194, 79...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Kesko , the company agreed with t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[101, 2429, 2000, 17710, 21590, 1010, 1996, 21...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  The Efore plant at Saarijarvi in central Finla...      1   \n",
       "1  The companies will divest to UPM Fray Bentos p...      1   \n",
       "2  The tower 's engineers have created an 18 degr...      1   \n",
       "3  Finnish silicon wafer technology company Okmet...      1   \n",
       "4  According to Kesko , the company agreed with t...      2   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1996, 1041, 29278, 2063, 3269, 2012, 784...   \n",
       "1  [101, 1996, 3316, 2097, 11529, 3367, 2000, 203...   \n",
       "2  [101, 1996, 3578, 1005, 1055, 6145, 2031, 2580...   \n",
       "3  [101, 6983, 13773, 11333, 7512, 2974, 2194, 79...   \n",
       "4  [101, 2429, 2000, 17710, 21590, 1010, 1996, 21...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tokenized_dataset[\"test\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a692ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Efore plant at Saarijarvi in central Finla...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The companies will divest to UPM Fray Bentos p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The tower 's engineers have created an 18 degr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finnish silicon wafer technology company Okmet...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Kesko , the company agreed with t...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>`` After this purchase , Cramo will become the...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>According to HKScan Finland , the plan is to i...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>According to Arokarhu , some of the purchases ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Barclays Plc ( LSE : BARC ) ( NYSE : BCS ) , C...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There will be return flights from Stuttgart ev...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  predicted_label\n",
       "0  The Efore plant at Saarijarvi in central Finla...      1                1\n",
       "1  The companies will divest to UPM Fray Bentos p...      1                1\n",
       "2  The tower 's engineers have created an 18 degr...      1                1\n",
       "3  Finnish silicon wafer technology company Okmet...      1                1\n",
       "4  According to Kesko , the company agreed with t...      2                1\n",
       "5  `` After this purchase , Cramo will become the...      2                1\n",
       "6  According to HKScan Finland , the plan is to i...      2                2\n",
       "7  According to Arokarhu , some of the purchases ...      0                1\n",
       "8  Barclays Plc ( LSE : BARC ) ( NYSE : BCS ) , C...      1                1\n",
       "9  There will be return flights from Stuttgart ev...      1                1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = df[[\"sentence\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "df[\"sentence\"] = df[\"sentence\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# Add the model predictions to the dataframe\n",
    "predictions = trainer_loaded_model.predict(tokenized_dataset[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       True\n",
      "1       True\n",
      "2       True\n",
      "3       True\n",
      "4      False\n",
      "       ...  \n",
      "771     True\n",
      "772     True\n",
      "773     True\n",
      "774     True\n",
      "775     True\n",
      "Length: 776, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "score = df[\"label\"] == df[\"predicted_label\"]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8518041237113402"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.sum() / len(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
